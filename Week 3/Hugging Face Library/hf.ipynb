{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff94bd83",
   "metadata": {},
   "source": [
    "to install `pip install transformers datasets tokenizers torch`\n",
    "to verify:\n",
    "```python\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bcc034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3258e8",
   "metadata": {},
   "source": [
    "We will be visiting the core libraries now.\n",
    "\n",
    "1. Transformers: Provides pre-trained models and pipelines for tasks like text classification.\n",
    "For eg., sentiment analysis.\n",
    "2. Datasets: Access and preprocess datasets efficiently.\n",
    "For eg., loading the IMDB dataset.\n",
    "3. Tokenizers: Efficiently tokenize text for model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d138020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9997791647911072}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\") # default model is distilbert-base-uncased-finetuned-sst-2-english\n",
    "print(classifier(\"i am not very happy right now\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2402d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169e5254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ANITA\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010, 17662,  2227,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer(\"Hello, Hugging Face!\", return_tensors=\"pt\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fd0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Basic Model Loading\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06166347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998494386672974}]\n"
     ]
    }
   ],
   "source": [
    "#Model Inference with Pipeline\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "print(classifier(\"Great tutorial!\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92acfbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9995478987693787}]\n"
     ]
    }
   ],
   "source": [
    "#NLP (Text Classification)\n",
    "classifier = pipeline(\"text-classification\")\n",
    "print(classifier(\"confusing movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f93554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'lynx, catamount', 'score': 0.7097601890563965}, {'label': 'Egyptian cat', 'score': 0.14048300683498383}, {'label': 'tabby, tabby cat', 'score': 0.07001744955778122}, {'label': 'tiger cat', 'score': 0.022446582093834877}, {'label': 'Siamese cat, Siamese', 'score': 0.008534948341548443}]\n"
     ]
    }
   ],
   "source": [
    "vision_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "print(vision_classifier(\"https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c31f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Fine-tuning a Model, Step 1: Load Dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7fbe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Load Model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d439c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4384.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Preprocess\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bd0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (4.53.0)\n",
      "Requirement already satisfied: filelock in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.1 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from transformers[torch]) (1.8.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.0)\n",
      "Requirement already satisfied: psutil in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: setuptools in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from torch>=2.1->transformers[torch]) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\iitb\\learner space 2025\\intro-to-ml-and-nlp\\hf_env\\lib\\site-packages (from requests->transformers[torch]) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678d971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 01:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=0.3260621428489685, metrics={'train_runtime': 100.8839, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.04, 'total_flos': 13246739865600.0, 'train_loss': 0.3260621428489685, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4: Train the Model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "small_dataset = tokenized_dataset[\"train\"].shuffle().select(range(100)) \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./quick_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=small_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc01fa",
   "metadata": {},
   "source": [
    "# Model Sharing\n",
    "Share models on the Model Hub.\n",
    "\n",
    "1. Log in:\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login()  # Use your Hugging Face token\n",
    "```\n",
    "2. Push model:\n",
    "```python\n",
    "model.push_to_hub(\"my-model\")\n",
    "tokenizer.push_to_hub(\"my-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ced9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Save the Model\n",
    "model.save_pretrained(\"./fine_tuned_model_small_dataset\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
