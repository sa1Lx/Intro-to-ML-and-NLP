{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f4f958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Model moved to CUDA successfully.\n",
      "487.46875\n",
      "542.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "model.to(\"cuda\") # Model loaded below first\n",
    "print(\"Model moved to CUDA successfully.\")\n",
    "print(torch.cuda.memory_allocated(0) / (1024 ** 2))\n",
    "print(torch.cuda.memory_reserved(0) / (1024 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d90c2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "ds_builder = load_dataset_builder(\"wikitext\", \"wikitext-2-v1\") # was too large to train so just used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6534a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: ~12.71 MB\n",
      "Dataset description: \n",
      "\n",
      "Features:\n",
      " {'text': Value(dtype='string', id=None)}\n",
      "\n",
      "Splits:\n",
      " ['test', 'train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "total_size = sum(split.num_bytes for split in ds_builder.info.splits.values()) / (1024 ** 2)\n",
    "print(f\"Total dataset size: ~{total_size:.2f} MB\")\n",
    "print(\"Dataset description:\", ds_builder.info.description)\n",
    "print(\"\\nFeatures:\\n\", ds_builder.info.features)\n",
    "print(\"\\nSplits:\\n\", list(ds_builder.info.splits.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18363509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "ds_builder_alt = load_dataset_builder(\"stas/openwebtext-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1ab796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: ~47.37 MB\n",
      "Dataset description: An open-source replication of the WebText dataset from OpenAI.\n",
      "\n",
      "This is a small subset representing the first 10K records from the original dataset - created for testing.\n",
      "\n",
      "The full 8M-record dataset is at https://huggingface.co/datasets/openwebtext\n",
      "\n",
      "\n",
      "Features:\n",
      " {'text': Value(dtype='string', id=None)}\n",
      "\n",
      "Splits:\n",
      " ['train']\n"
     ]
    }
   ],
   "source": [
    "total_size = sum(split.num_bytes for split in ds_builder_alt.info.splits.values()) / (1024 ** 2)\n",
    "print(f\"Total dataset size: ~{total_size:.2f} MB\")\n",
    "print(\"Dataset description:\", ds_builder_alt.info.description)\n",
    "print(\"\\nFeatures:\\n\", ds_builder_alt.info.features)\n",
    "print(\"\\nSplits:\\n\", list(ds_builder_alt.info.splits.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36059f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_builder.download_and_prepare()\n",
    "ds = ds_builder.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850fb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_builder_alt.download_and_prepare()\n",
    "ds_alt = ds_builder_alt.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28e5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d787575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=256, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ef654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(example): # for measuring performance\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e6b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3546bec1dd994b9f84b5ee1bf44d438a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad228ed89324d36b8edb6a8265af00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f81e2937e2342f7a1319f6f554073b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = ds.map(tokenize_test, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426886e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c227af0cd3d14bdfa75e08c28093b80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_alt = ds_alt.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8cba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_alt = tokenized_dataset_alt[\"train\"].train_test_split(test_size=0.2, seed=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66245fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Shift so model predicts token t+1\n",
    "    shift_logits = torch.tensor(logits)[..., :-1, :].contiguous()\n",
    "    shift_labels = torch.tensor(labels)[..., 1:].contiguous()\n",
    "\n",
    "    # Flatten the tensors\n",
    "    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "\n",
    "    # Mask out padding\n",
    "    valid = shift_labels != -100\n",
    "    y_true = shift_labels[valid].numpy()\n",
    "    y_pred = shift_logits[valid].numpy()\n",
    "\n",
    "    # Perplexity (cross-entropy)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "    perplexity = math.exp(loss.item())\n",
    "\n",
    "    # Top-k accuracy\n",
    "    topk_acc = top_k_accuracy_score(y_true, y_pred, k=5, labels=list(range(50257))) # GPT-2 vocab size is 50257\n",
    "\n",
    "    return {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"top5_accuracy\": topk_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d4bb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06afba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 16:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.033410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.131700</td>\n",
       "      <td>3.029742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=3.13165234375, metrics={'train_runtime': 976.1486, 'train_samples_per_second': 16.391, 'train_steps_per_second': 0.512, 'total_flos': 2090336256000000.0, 'train_loss': 3.13165234375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./NWP_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_alt[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_alt[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10afe790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned-nwp\\\\tokenizer_config.json',\n",
       " './gpt2-finetuned-nwp\\\\special_tokens_map.json',\n",
       " './gpt2-finetuned-nwp\\\\vocab.json',\n",
       " './gpt2-finetuned-nwp\\\\merges.txt',\n",
       " './gpt2-finetuned-nwp\\\\added_tokens.json',\n",
       " './gpt2-finetuned-nwp\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./gpt2-finetuned-nwp\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned-nwp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55cee596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned-nwp\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-finetuned-nwp\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc42c378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 95/100 00:05 < 00:00, 16.10 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.30 GiB for an array with shape (96, 128, 50257) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m trainer = Trainer(\n\u001b[32m      8\u001b[39m     model=model,\n\u001b[32m      9\u001b[39m     args=training_args,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m outputs = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env311\\Lib\\site-packages\\transformers\\trainer.py:4278\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4275\u001b[39m start_time = time.time()\n\u001b[32m   4277\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4278\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPrediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[32m   4280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4281\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env311\\Lib\\site-packages\\transformers\\trainer.py:4447\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4445\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m args.eval_accumulation_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (step + \u001b[32m1\u001b[39m) % args.eval_accumulation_steps == \u001b[32m0\u001b[39m:\n\u001b[32m   4446\u001b[39m     all_losses.to_cpu_and_numpy()\n\u001b[32m-> \u001b[39m\u001b[32m4447\u001b[39m     \u001b[43mall_preds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_cpu_and_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4448\u001b[39m     all_labels.to_cpu_and_numpy()\n\u001b[32m   4449\u001b[39m     all_inputs.to_cpu_and_numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env311\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:332\u001b[39m, in \u001b[36mEvalLoopContainer.to_cpu_and_numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays = new_arrays\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_nested_concat:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays = \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     \u001b[38;5;28mself\u001b[39m.arrays.extend(new_arrays)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env311\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:137\u001b[39m, in \u001b[36mnested_concat\u001b[39m\u001b[34m(tensors, new_tensors, padding_index)\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[32m    134\u001b[39m         {k: nested_concat(t, new_tensors[k], padding_index=padding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors.items()}\n\u001b[32m    135\u001b[39m     )\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, np.ndarray):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for concatenation: got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\IITB\\Learner Space 2025\\Intro-to-ML-and-NLP\\hf_env311\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:107\u001b[39m, in \u001b[36mnumpy_pad_and_concatenate\u001b[39m\u001b[34m(array1, array2, padding_index)\u001b[39m\n\u001b[32m    104\u001b[39m array2 = atleast_1d(array2)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array1.shape) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m array1.shape[\u001b[32m1\u001b[39m] == array2.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[32m    110\u001b[39m new_shape = (array1.shape[\u001b[32m0\u001b[39m] + array2.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(array1.shape[\u001b[32m1\u001b[39m], array2.shape[\u001b[32m1\u001b[39m])) + array1.shape[\u001b[32m2\u001b[39m:]\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 2.30 GiB for an array with shape (96, 128, 50257) and data type float32"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "test_ds = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./dummy\", per_device_eval_batch_size=1, fp16=True, eval_accumulation_steps=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "outputs = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b84468b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2553055600 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m outputs.predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mPredictions are None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m outputs.label_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mLabel IDs are None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m metrics = \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluation Metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m metrics.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(eval_pred)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Perplexity (cross-entropy)\u001b[39;00m\n\u001b[32m     23\u001b[39m loss_fct = torch.nn.CrossEntropyLoss()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m loss = loss_fct(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m, torch.tensor(y_true))\n\u001b[32m     25\u001b[39m perplexity = math.exp(loss.item())\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Top-k accuracy\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2553055600 bytes."
     ]
    }
   ],
   "source": [
    "assert outputs.predictions is not None, \"Predictions are None\"\n",
    "assert outputs.label_ids is not None, \"Label IDs are None\"\n",
    "metrics = compute_metrics((outputs.predictions, outputs.label_ids))\n",
    "print(\"Evaluation Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0fcb1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like watching the movies\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I like watching\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=2)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
